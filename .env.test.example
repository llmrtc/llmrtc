# =============================================================================
# E2E Test Environment Variables
# Copy this file to .env.test and fill in the values
# =============================================================================

# =============================================================================
# Cloud Provider API Keys (for provider integration tests)
# Tests will be skipped if the corresponding API key is not set
# =============================================================================

# OpenAI - LLM, Whisper STT, TTS
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# ElevenLabs - TTS
# Get your key at: https://elevenlabs.io/
ELEVENLABS_API_KEY=xi-...

# Anthropic - Claude LLM
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# Google - Gemini LLM
# Get your key at: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=AIza...

# AWS Bedrock - Multi-model LLM
# Configure via AWS IAM with Bedrock permissions
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# OpenRouter - Multi-model gateway
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-...
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# =============================================================================
# Local Services (URLs)
# These are for LOCAL_ONLY mode testing with local AI services
# =============================================================================

# Ollama - Local LLM
# Install: https://ollama.ai/
# Run: ollama serve (then: ollama pull llama3.2)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# LMStudio - Local LLM
# Download: https://lmstudio.ai/
# Run: Launch app, load model, start server
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model

# Faster Whisper - Local STT
# Run: docker run -p 8000:8000 fedirz/faster-whisper-server:latest
FASTER_WHISPER_URL=http://localhost:8000

# Piper - Local TTS
# Run: docker run -p 5000:5000 rhasspy/piper
PIPER_URL=http://localhost:5000

# =============================================================================
# Test Server Configuration
# =============================================================================

# Backend server for tests
TEST_BACKEND_PORT=8788
TEST_BACKEND_HOST=127.0.0.1

# Frontend dev server URL
TEST_FRONTEND_URL=http://localhost:5173

# WebSocket signalling URL
TEST_SIGNALLING_URL=ws://127.0.0.1:8788

# =============================================================================
# Test Modes
# =============================================================================

# Set to 'true' to only test with local providers (Ollama, Piper, etc.)
# Set to 'false' or omit to test with cloud providers
LOCAL_ONLY=false
